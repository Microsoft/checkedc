% !Tex root = checkedc.tex

\chapter{Related work}
\label{chapter:lessons}

The C family of programming language is widely used.  The lack of
bounds checking in C and related languages such as C++ has had
serious practical consequences for computer security and software 
reliability.   There has been extensive work in industry and the
research community addressing the lack of bounds checking in C.
In this section, we discuss related work,  
highlighting key lines of work and describing how Checked C relates to them. 

We begin by discussing the most closely related work to Checked C:
type-safe dialects of C.  These address the underlying problem of
a lack of bounds checking as well as other aspects of type safety.
These dialects are not used in practice, so we look closely at why
this is true for lessons for the Checked C effort.

We then discuss security mitigations that address
security problems that arise from a lack of bounds checking.
These mitigations are widely used in practice.   Checked C
complements these mitigations by providing protection against data
modification and data disclosure attacks.

\section{Runtime-only approaches}

The C specification leaves the semantics of out-of-bounds pointer
arithmetic undefined, except for the case of a pointer to one element
past the last element of an array.  It also leaves the semantics of out-of-bounds
pointer access undefined.  This makes it possible to implement
runtime-only bounds checking that is consistent with the semantics of
C and that dooes not require source code changes.   It is also possible
use probabilistic or fault-tolerant approaches to tolerate out-of-bounds
memory accesses.

There are two general approaches used 
for runtime-only bounds checking.
The first approach is to change the representation of pointers to carry
bounds with them.  The second approach uses a side-data structure to
hold bounds information.   There are several different kinds of side-data
structures used.  One kind tracks blocks of memory that are valid to
use.  The blocks are typically powers of 2 and can range in size
from a byte to up to 32 bytes. This is used to track ``red zones'' around objects that
are invalid to access.  These are useful for detecting buffer overruns caused
by loops, but may not detect other out-of-bounds access.  Another kind tracks the
start and end locations of objects and can be used to provide object-level bounds checking.   A third approach uses a shadow memory to track bounds information for individual pointers, splitting the bounds information from the pointer.

These approaches work well for testing purposes, but they have issues that
prevent them from being used for production systems.  There are issues
with performance, backwards-compatibility, and constraints on memory
management and layout.  Most fundamentally, all of these approaches add
runtime data to programs, which in turn increases memory accesses, processing 
times, and memory footprint.  They have to pay for generality regardless of
whether it is needed or not. In contrast, Checked C allows re-use of existing data.
It incurs no data overhead for constant-sized arrays or pointers on which no pointer arithmetic is done.   Approaches that change
pointer representations have difficulty interoperating with existing
systems because that changes the layout of data that must be passed
across boundaries.  Approaches that track data on the side generally
requiring hooking memory allocators.  Furthermore, a number of them
constrain memory layout and object sizes to speed up the lookup of
the side data.

These approaches also take control of checking away from the programmer.
Runtime-only checking {\em always} checks, unless the check can be proven 
redundant by compiler optimization.   With Checked C, programmers have
a range of options   The default behavior is to always check. For performance-critical
code, programmers can rely on static checking that proves the checks
are unnecessary or omit checks entirely.

We first describe approaches that change the representation of pointers.
The bcc source-to-source translator \cite{Kendall1983} and the rtcc
compiler \cite{Steffen1992} were used to find bounds errors and
other errors during debugging.  Each changed the representations of pointers to be 
3 words: the pointer itself and an upper and lower bound.   
Steffen \cite{Steffen1992} reports that the rtcc compiler generated code
that was 3 times larger and ran about 10 times slower than the original code, 
likely reflecting the simple nature of  the optimizer for the PCC compiler.  Data
layout compatability is an issue.  Bounds information has to be removed at calls to
standard library  functions and added at returns from standard library functions.
Austin {\it et al.} \cite{Austin1994} describe a pointer representation that
adds a capability in addition to bounds information.  The capability prevents
accesses to de-allocated objects.  The runtime system tracks capabilities that
are valid (memory that has not been deallocated).  

Fail-Safe C is a memory-safe compiler for ANSI C \cite{Oiwa2009}.  It
supports all operations in ANSI C,including cast operations.
It represents pointers as pairs, where each pair consist of the base address
of an object and an integer offset from the base address. 
It changes the representation of integers to be pairs as well so that pointers can 
be cast to integers and back.  It also changes the representation of memory blocks in
C to dynamically track their types.  This
supports the C notion that memory locations are dynamically typed; the type
of the value in a memory location depends on the type of the last value stored there.
It use conservative garbage collection to ensure the safety of memory
deallocations.   With the data layout changes, programmers no longer 
directly control memory representations.  This makes Fail-Safe C unsuitable for low-level  systems programming.
It is suitable for applications programs, provided the wrappers for system
calls are provided.  Bytemark benchmarks are 2 to 4 times slower.  OpenSSL RSA
speed tests are 2 to 4 times slower, while AES speed tests are 5 times slower.

Grimmer {\it et al.} \cite{Grimmer2015} propose executing C programs within a Java
Virtual Machine.   They represent pointers as pairs consting of the base address of an
object and the element count offset from the base address.  They do not allow
pointers produced by casting integers to pointers to be used to access
memory.

We next describe approaches that use side-data structures.    We first
describe approaches that track what memory is valid to access.
Purify \cite{Hastings1992, Unicom2016} detects some bounds checking problems
as well as uses of uninitialized memory and memory leaks.  It is meant for
use during development and debugging.  Purify uses a table that keeps track
of the state of each byte in memory, using 2-bits to represent the state of
memory.  It inserts a small ``red zone'' before and after each dynamically
allocated object and between statically-allocated objects.   It also
inserts red zones between stack frames.  All memory accesses are instrumented 
to  check or update the state for a byte. 
The instrumentation is inserted by rewriting binary code before linking.
Purify detects out-of-bounds memory reads and writes involving red-zone
memory.   It does not detect out-of-bounds
reads or writes that occur entirely within valid memory for other objects
or stack frames. It cannot detect out-of-bounds memory accesses at
the sub-object level.
Hastings {\it et al.} report a slowdown of more than 2 times due to
the instrumentation.  However, this includes additional checking for memory leaks 
and the use of uninitialized memory.

There are a number of other similar commercial or open-source tools available
that detect out-of-bounds memory accesses.
Tools based on binary rewriting include Bounds Checker \cite{BoundsChecker2016}, 
Dr. Memory \cite{Bruening2011,DrMemory2016}, Intel Inspector \cite{Intel2016},
Oracle Solaris Studio Code Analyzer \cite{CodeAnalyzer2016},
and Valgrind Memcheck  \cite{Nethercote2007, Valgrind2016}.  Insure++ 
\cite{Insure2016}
inserts instrumentation using source-to-source rewriting.  It also provides
a mode that does not require recompilation, although details of how that
works are not described.

AddressSanitizer \cite{Serebryany2012} is a tool similar to Purify that is
incorporated into the LLVM and GCC compilers.  It uses a table
stored in shadow memory that tracks that state of 8-byte chunks of memory, using
1/8 of the virtual address space.  It also relies on inserting ``red zones''.
Because it has been implemented in a compiler, it is able to place red zones
around stack-allocated objects.   It cannot detect out-of-bounds memory
accesses at the sub-object level and cannot reliably track out-of-bounds accesses
for objects smaller than 8 bytes.  For SPEC CPU2006, average program execution time
increases by 73\% when checking reads and writes and 26\%
when only checking writes. For SPEC CPU2006, average memory usage is 
3.37 times larger.

Light-weight Bounds Checking \cite{Hasabnis2012} is an optimized implementation of bound
checking that uses ``red zones''.
it focuses solely on bounds checking.  It uses a bitmap to track
which bytes in memory correspond to allocated objects and which do not.
It uses a two-level table to avoid allocating a table equal to 1/8 of the
address space.  It optimizes memory reads by filling red zones with special
values.  If a memory read returns a special value, then a check is done to
ensure that the address was not a red zone address.  It is implemented in a compiler and can guard stack objects.  For SPECINT 2000, average 
program execution time increases by 23\%. For SPECINT 2000, its memory 
overhead ranges between 0.2\% and 44\% with an average of 8.5\%.

Next, we describe approaches that use side-data structures to track
object bounds.   
Jones and Kelly \cite{Jones1997} use a splay tree to track the bounds for
objects in a side data structure.  They insert 
checks for pointer arithmetic to make sure that pointers stay within valid bounds for
objects.  If a pointer goes out of bounds, it is converted to a value that cannot
be dereferenced and that is not allowed to go back in bounds. 
They change checked code to call modified versions of \keyword{malloc}
and \keyword{free}  as well as system-level allocator.  The compiler modifies
code generation for stack allocate objects to call funtions that update the 
bounds information. In a production system, all custom allocators would have to be
modified to update the object bounds information.  

Jones and Kelly implememented their approach in GCC.   The approach easily allows
interoperation between checked and unchecked code.  Objects that are not allocated by
checked code are not tracked.  Untracked and tracked objects are treated as distinct memory regions.  Operations in checked code on untracked objects are not allowed to produce pointers to checked objects, providing some protection to untracked objects.  

Their implementation increases program running
times by a factor of 7.7 to 12 \cite{Nagarakatte2009} for  a set of
23 benchmarks that includes programs from SPEC and the Olden benchmark.
There are other drawbacks to this approach.
Because bounds are tracked at the granularity of objects, it
cannot track bounds for arrays nested within structures.  It also cannot
track bounds for objects produced by custom allocators.  There are also
issues with handling pointers to one element past the last element of
an array.  The approach inserts padding between objects to have a gap.  In some
cases, such as parameters passed on the stack, this is not possible.

The C Range Error Detector (CRED) \cite{Ruwase2004} extends the Jones and Kelly
approach to tolerate out-of-bounds pointers.  They observe that 12 out of
20 open source programs totalling 1.2 million lines of C code break when
out-of-bounds pointers are not allowed.   This observation supports the
design decision in Checked C to allow out-of-bounds pointers and only
check bounds at memory dereferences.

CRED uses a proxy object that
tracks the original pointer value and the object with which it is associated
os that the pointer can go in back bounds.  When a pointer goes out of
bounds, it is replaced with the proxy object address.  This requires
additional checks on comparisons and pointer arithmetic computations.
They find similar increase in program running times.  They suggest
limiting bounds checking to only character pointers. With this restriction,
the increase in execution time  ranges from 1\% to 130\% on a set of real-world
programs.  They do not report on changes in memory usage.

Dhurjati and Adve \cite{Dhurjati2006} describe an optimized implementation of
CRED.  It relies on a whole-program analysis to partition
objects into separate pools.  The pool information is used to partition
the splay tree and  to avoid having to create entries for single-element arrays 
or scalar objects in the splay tree.

Baggy Bounds Checking \cite{Akritidis2008} provides a faster implementation
of the side data structure in Jones and Kelly.  The implementation 
calculates the bounds for any pointer in constant-time.   To acheive this,
the implementation constrains object sizes to be powers of 2.  It also reserves 
1/var{n} of the virtual address space for a table, where \var{n} is the smallest 
allowed object size (16 in the implementation).  The table stores the size of the
enclosing object (if any) for each \var{n}-byte chunk of memory.   Baggy Bounds 
Checking increases the average execution time for SPECINT 2000 benchmarks by 60\%. 
It increases average memory usage by 20\%.

Paricheck \cite{Younan2010} stores a 2-byte label for each 32-byte chunk of memory.
It reserves 1/16 of the virtual address space for a table and increases
the memory object allocation size to 32 bytes.
It checks that pointer arithmetic stays in bounds by checking that the
original pointer and the pointer computed by pointer arithmetic have the
same label. For SPEC CPU2000, it increases average execution time 49\% and
average  memory usage by 9.5\%.

Low Fat Pointers \cite{Duck2016} encode bounds information into 64-bit pointers by
dividing memory into \var{m} regions of 4 GBytes each and storing
the region information in the upper 32-bits of the pointer.  Each
region contains objects of some size $k$ that are aligned to $k$.
A table maps regions to their sizes.  They measure the performance of 
SPEC 2006 programs and find that checking all pointer reads and writes
increases average program execution time by 56\% and checking only writes 
increases average execution time by 13\%.

Finally, we describe shadow memory approaches.
Patil and Fischer \cite{Patil1997}
implement bounds checking using a second process that follows the execution 
of the original process.  They separate the bounds and lifetime information
from the original pointer and use shadow heaps and shadow stacks in the
second process to track it.  
SoftBound \cite{Nagarakatte2009} tracks bounds information
for each  memory location that contains a pointer by using a side table.  
As an example, given a pointer variable,
the system tracks the bounds based on the address of the pointer variable.
This allows the system to track sub-bounds within objects.  The calling
convention for values passed in registers is modified to have an additional
parameters for bounds.  SoftBound uses either a hash table or
a shadow copy of memory to track the bounds information.   The shadow
copy of memory generally has better performance.  With the shadow
copy of memory, SoftBound increases average program execution time for
a set of benchmarks by 67\%.   SoftBound can check only writes, in which
case average program execution time increases by 22\%.   SoftBound increases 
average memory footprint by 64\%, although it can increase it by up to 200\%.

MemSafe \cite{Simpson2013} provides bounds safety and lifetime safety for C.
It too tracks bounds information for each memory location that contains a 
pointer using a side table.  It uses capabilities to prevent the use of
invalid pointers to access or free memory.  It stores capabilities in the side 
table and keeps a map from valid capabilities to the bounds of the associated objects.
MemSafe uses whole-program optimization to optimize dynamic checks, to avoid placing bounds and capability information in the side table, and to avoid tracking 
capabilities. For 30 programs from the Olden, 
PtrDist, and SPEC benchmarks, MemSafe increases average execution time by 88\%
and average memory usage by 49\%.

An alternate approach to dealing with out-of-bounds memory accesses is to
use probabilistic techniques that allow programs to tolerate out-of-bounds
memory writes.   DieHard \cite{Berger2006} randomizes the location and spacing of 
objects on the heap.  It allocates a heap at least as twice as large as needed for
program data.    This causes out-of-bounds writes to modify memory in gaps
between objects.  This provides good protection against modest buffer overflows. 
To protect against out-of-bounds memory reads, programs
can be replicated, run in parallel, and text outputs compared.  The replicated
approach does not work for programs that have non-deterministic output. It is 
unclear how it would work for programs that are interactive.

\section{Security mitigations}
Security mitigations are another approach to dealing with programs that corrupt memory
through out-of-bounds writes or the expose data through out-of-bounds
read.   These employ runtime-only mechanisms to detect that memory has been
corrupted or to prevent something bad from happening after memory has been
corrupted.

Attackers can use incorrect programs to attack the security of 
computer systems in the following ways:
\begin{enumerate}
\item  Execution of arbitrary code: An attacker may be able to inject arbitrary machine code into a process and have the process  execute that code.
\item Control-flow attacks: this a more subtle attack that avoids the need to inject arbitrary machine.  An attacker manipulates program state to
stitch together execution of a program of the attacker's choosing
using existing machine code.
For example, in return-oriented programming, an attacker finds segments of useful
machine code that end in return instructions. The attacker manipulates the state of 
the program call  stack to execute a series of small pieces of machine code and execute
a arbitrary program.  There are other ways to manpulate program state to
change control-flow, such as changing the target of an indirect function call.  
This can be done by 
ovewriting a function pointer or the virtual table of an object.
\item Data modification: an attacker may be able to write data to a process, causing the process to take an incorrect action on behalf of an attacker.
\item  Data disclosure: an attacker may be able to read data from a process 
and obtain data, including data that is meant to be confidential.  
\end{enumerate}

Security mitigations that have been developed and deployed in
practice include Data Execution Prevention (DEP), 
Address-Space Layout Randomization (ASLR), stack canaries,
shadow stacks, and Control-Flow Integrity (CFI).   DEP, ASLR, and CFI focus
on preventing execution of arbitrary code and control-flow modification.  
Stack protection mechanisms focus on protecting data or return addresses
on the stack. 

Checked C provides protection against data modification and
data disclosure attacks, which the other approaches do not.
Chen {\it et al.} \cite{Chen2005} show that data modification
attacks that do not alter control-flow pose a serious long-term threat.
The Heartbleed attack on OpenSSL illustrates the damage this is
possible from even data disclosure attacks.

Checked C addresses the fundamental problem, which are incorrect programs
with undetected errors. Checked C enhances existing security mitigations by
providing protection against data modification and data modification attacks.
ASLR, DEP, CFI, and stack canaries can be 
can be defeated by determined attackers using data modification and data disclosure
attacks.  Shadow stacks do not protect stack-allocated buffers or arrays, 
heap data, and statically-allocated data.

If a C program and its libraries use only checked pointers and checked arrays and 
the program is free of type 
cast or memory management errors, Checked C provides a strong
guarantee about memory reads and writes. Any pointer must have been constructed
via a  series of operations from a pointer to an object.   Checked C ensures 
that the constructed ponter only accesses memory within that object.  

Because Checked C addresses only bounds safety, Checked C programs 
remain vulnerable  to incorrect type casts, memory management errors, 
and race conditions that invalidate bounds information. They are also
vulnerable to data modification by unchecked code in the same address space.

In the remainder of this section, we review security mitigations that
have been deployed in practice. We discuss what they protect against
and they can be defeated by data modification and data disclosure attacks.

Data execution prevention (DEP) relies on hardware and OS-based approaches
to prevent attacks that inject machine code into a process and then
execute it.   At the hardware level, virtual memory support provides
a ``no execute'' bit for each virtual memory page that forbids the execution
of instructions located on that page of virtual memory.  This bit can be set by
default for program stacks and for memory when it is first allocated. A process
may have to request specifically that an area of virtual memory be made
executable.  
DEP does not defend against control-flow,  data modification, and data disclosure
attacks.  Control-flow attacks can be used to execute a system call and disable 
ndata execution protection.    

Data execution prevention is deployed widely in production.  It has been very
successful protecting against ``classic'' buffer overrun attacks that overwrite 
stack contents with machine code and also overwrite the return address to jump to 
the machine code.
For example, hardware support was incorporated into 32 and 64-bit processors
for the Intel x86 architecture in 2005 and 2003, respectively.  Windows
and Linux have supported it from that time as well.  For 32-bit programs on
Windows, programs must opt-in.  That is the default for programs compiled
with the Microsoft Visual C++ compiler, however.

Software can emulate ``no execute'' support or provide approximiations of it
when hardware support is not available. Software fault isolation
\cite{Castro2009, Erlingsson2006,McCamant2006,Wahbe1993,Yee2009} injects checks
into machine code, either during compilation or by rewriting binaries.
It implements address spaces in software for fault isolation. Software components communicate via remote-procedure calls. By implementing address
spaces in software, this avoids expensive hardware context switches.
Wahbe {\it et al} \cite{Wahbe1993} describe how execution of data can be
prevented by placing code and data in separate areas of memory and checking the
targets of  indirect jumps.  Code running in a software-fault isolated address
space, like code running in a hardware-supported address space, is vulnerable to 
control-flow attacks, data modification,  and data disclosure attacks.

ASLR \cite{PaX2003,WikipediaASLR} provides protection against 
control-flow attacks.   All major production operating systems provide
some form of ASLR. In ASLR, code
sections from executable files are loaded in random locations in the
address space of a process.  In addition, data sections, stacks, and
dynamically-allocated data are also placed in random locations in the address
space of a process. This makes it harder for an attacker to identify the locations of fragments of code to be used in return-oriented programming.  However, ASLR does
not protect against data modification or data disclosure attacks. For example, data 
may be located on the stack adjacent to a
variable that is subject to a buffer overrun; the buffer overrrun can be
be used reliability to overwrite or read the data.   

ASLR can be compromised by data disclosure attacks.  An attacker can obtain the
location of a code section by reading data from the stack, for example, and obtaining function return addresses.  The attacker can then craft an attack based on that data. 
A number of proposals suggested finer-grained  randomization of code layout
\cite{Bhatkar2005, Hiser2012, Kil2006, Pappas2012,Wartell2012}.
For example, code layout can be randomized at the instruction level or
basic-block level. These too are vulnerable to 
data disclosure attacks \cite{Snow2013}.

The idea behind ASLR is to use randomization to protect
code and data addresses.  For randomization of addresses to be effective, 
it requires hardware  architectures that have 64-bit addresses and 
virtual memory support \cite{Shacham2004}.  With 32-bit addresses, 
there is typically  at most 16 bits of entropy available for
virtual memory allocation.  Data is allocated at virtual memory
page granularities or multiples of virtual memory page granularities
and using upper bits fragments the virtual address space, which
could cause large virtual memory allocations to fail.  This
amount of entropy is not enough to defend systems deployed at scale on
the Internet.  An attacker only needs on average 32,768 probes for one system or 
conversely 32,768 target systems to compromise one of them through a
brute force attack.
This limits the usefulness of ASLR in embedded domains where 
64-bit address spaces are uncommon.  

Another set of ideas is to protect the data that contains 
code addresses.  Some ideas aim to protect against modification;
others aim to protect against disclosure of the code addresses.
Stack canaries \cite{Cowan1998, Dang2015, Petsios2015} provide 
some protection against injection of 
arbitrary code and control-flow attacks.
They protect against attacks that modify return addresses
on call stacks by overrunning the bounds of a stack-allocated array.  
This kind of overrun can happen when using C string functions that do
not valid parameter lengths, for example.   A
compiler injects code at function entry points and return
points. Entry point code places a token on the call stack
next to the return address.  Return point code checks that
the token has not been modified before executing the return
instruction.  The value of the token may be computed at run time
and selected to have specific properties that aid in the detection
of overflow attacks.  This form of stack protection is available as a
compiler option for the GCC and Microsoft Visual C++ compilers.

Canaries do not provide protection against data modification attacks
that modify only the contents of stack-allocated variables, that
precisely modify return addresses, or that modify other areas 
of memory such as the heap and global variables. They also do not
provide protection against data disclosure attacks. 

An alternative to stack canaries is shadow stacks \cite{Abadi2005, 
Baratloo2000, Bhatkar2005, Chiueh2001, Corliss2005, Erlingsson2006,
Frantzen2001, Kuznetsov2014}.
With shadow stacks,
the stack is split into two stacks.  One stack is the secure stack. It
is accessible via a dedicated hardware register and placed at a random
location in memory.   The location of the secure stack is protected
against disclosure by guaranteeing that all memory accesses
to the secure stack are in bounds.  For example, only scalar variables whose
addresses are not taken may be stored on the secure stack. In addition, 
the addresses of locations on the secure stack are only stored on the secure
stack.   The other stack is the insecure stack.  Its location is not
protected against disclosure.

In some approaches, the secure stack is used to only hold return addresses.
A return address is stored on the regular stack as well and there is a
check that the return address is unmodified before doing a return.
This incurs an overhead of about 10\% because of the cost of having two
stacks and checking return addresses \cite{Dang2015}.   The
the regular stack can be used as the secure stack and that variables
that may have out-of-bounds memory accesses or whose addresses are taken
can be stored on the insecure stack \cite{Bhatkar2005, Kuznetsov2014}.  
Kuznetsov {\it et al.}.   
\cite{Kuznetsov2014} observe that this improves performance because
many small stack frames do not even need shadow stack frames.  For
SPEC CPU 2006 benchmarks, they found that the performance cost 
ranges from -4.2\% to 4.1\%,
with an average cost of less than 0.1\%.  They speculate
that performance improvements are due to improved data locality for
stack accesses from the placement of large arrays on the insecure stack.
This form of stack protection is available as a compiler option 
for the LLVM compiler.

Shadow stacks do not prevent data modification and data disclosure attacks
against the insecure stack or other areas of memory such as the heap 
and global variables.  Shadow stacks also have backwards compatibility
problems; all code used by a thread must be converted to prevent disclosure
of the shadow stack location for the thread.  

A shadow stack that uses the regular stack can be attacked in
a subtle way, even if all memory accesses to the stack
are guaranteed to be in bounds.  An attacker can cause a calling convention
mismatch, where the caller of a function and the called function disagree on
the size of argument data that is passed on the stack or who is expected to
make adjustments to the stack pointer.  This corrupts the stack pointer, 
allowing a data modification or data disclosure attack against the shadow stack, including overwriting return addresses \cite{Goktas2014}.

Like ASLR, shadow stacks that use the regular call stack may be vulnerable to 
brute-force data disclosure attacks on systems with 32-bit addresses.
For example, on a 32-bit Windows system, the smallest possible
stack size is 64K and the uppermost 1 GByte of virtual address space
is not available by default.  If an attacker is able to read a byte 
in memory at an attacker-selected location and the attack randomly picks
the location,  an attacker has a 1 in 49,152 
chance of reading a byte that is on a virtual memory page that contains a 
shadow stack location.  With reads of nearby locations, an attacker can likely 
determine if the page containing the byte contains part of a stack.

Control-flow integrity (CFI) also provides some protection
against return-oriented programming attacks \cite{Abadi2005}.  There
have been many follow-up papers \cite{Akritidis2008, Li2011, Li2010, Mashtizadeh2015,
NiuPLDI2014, NiuCCS2014, Niu2015, Sadeghi2015, Tice2014, 
vanderVeen2015, Wang2015, Wang2010, Zeng2011, ZhangSP2013,Zhang2015, ZhangSEC2013}.
CFI adds runtime checks to machine code to ensure
that a program follows an approximation of the valid control-flow of the program. 
In the description in \cite{Abadi2005}, a compiler computes the target addresses of each 
indirect  function call and each return instruction.  The
compiler groups addresses into equivalence classes: addresses are in the same 
class if they may be the target of the same call or return instruction.  This is
used to generate unique identifiers for the runtime control-flow check. 
Many different variants of CFI have been proposed, including coarse-grained CFI
implementations that are less precise than the original description \cite{Wang2015,
ZhangSP2013, ZhangSEC2013} 
as well as fine-grained CFI implementations \cite{Tice2014,Wang2010} 
and even context-sensitive CFI implementations \cite{vanderVeen2015}.
CFI has been applied in production C and C++ compilers to indirect function
calls and not applied to return instructions
\cite{GCCCFG2016,LLVMCFG2016,MicrosoftCFG2016, Tice2014}.   Return instructions
are protected via other means such as stack canaries or shadow stacks.  
CFI is implemented in various forms in production versions of the
GCC, LLVM, and Microsoft Visual C++ compilers.

CFI does not defend against data modification or data disclosure attacks. 
It is also vulnerable to data modification attacks.  Determined attackers can
use a data modification attack to still construct a control-flow attack
\cite{Carlini2015, Conti2015, Evans2015, Goktas2014}.
The runtime control-flow checks are imprecise because the control-flow graph (CFG)
is an approximation of the actual control-flow that is possible for
a program.  The computed CFG must allow at least all legal executions of a
program. In fact it allows invalid executions of a program too.  The
attacker can take advantage of that difference to control execution of the program
via a data modification attack.

CFI is based on the assumption that a precise control-flow graph can be
constructed for C and C++ programs.  According to Evans {\it et al.} \cite{Evans2015},
``this assumption is tenuous at best''.  It is difficult to construct
a precise CFG for programs with pointers that use function pointers and
object-oriented language features.  Coarse-grained CFI implementations make the
checked CFG even less precise.  They do this to reduce  the cost of runtime checking or 
because of difficulties computing a precise CFG.  For example, binary rewriting
approaches have difficulty precisely computing possible targets for indirect jumps.
This allows even more invalid executions.  Coarse-grained CFI implementations
were  first shown to be vulnerable to an 
attack based on the imprecision of the checks \cite{Carlini2014,Davi2014,Goktas2014}. 
Fine-grained CFI implementations are also vulnerable to the same kind of attack
\cite{Carlini2015,Evans2015}.

Evans {\it et al} \cite{Evans2015} explain how constructing the CFG relies on a
points-to analysis for pointers.  Sound and complete points-to analysis is 
undecidable \cite{Ramalingam1994}, so points-to analyses implemented in compilers 
must approximate the the actual points-to behavior of programs.  This leads to 
imprecise CFGs in practice.

It is also difficult to construct a precise CFG in the presence of 
separately-compiled dynamically-loaded libraries \cite{NiuPLDI2014} or just-in-time
compiled code \cite{NiuCCS2014}.  Tice {\it et al.} \cite{Tice2014} discuss the difficulties of implementing CFI for programs that use dynamically-loaded libraries. 
Hand-written assembly code also causes problems for constructing a precise CFG.

\section{Static analysis tools}

Static analysis tools are used widely to identify defects in C and C++ programs.
The tools take the source code for a program (or, less frequently, binary
code) and attempt to find possible bugs.  They do so by analyzing the source
code for the program.  There are many available commercial static analysis tools
available for C and C++, incuding CodeSonar, Coverity Static Analysis, HP Fortify, 
IBM Security AppScan Klocwork, Microsoft Visual Studio Code Analysis for C/C++, and Polyspace Static Analysis \cite{Bessey2010,Bush2000,Emanuelsson2008}.    
These tools find many different kinds of bugs, including out-of-bounds array accesses.

Static analysis tools use several different approaches to identify bugs,
including dataflow analysis \cite{Aho2007,Shahriar2010}, simulating program 
execution \cite{Bush2000},  abstract interpretation \cite{Cousot1977}, 
and model checking
\cite{Larus2004}. At a high level, they build a  model of the program and 
prove properties about the model.  This is a large area of study and describing 
it is well beyond the scope of this related work section. We focus only on how
Checked C relates to static analysis tools.

Static analysis tools are imprecise.   They may report that a program
may have a defect, when it does not have that defect.  The imprecision
is inherent in static analysis.  Deciding whether a program has a specific defect is
undecidable in general.  When a static analysis tool reports that a program has a 
defect  and it does not, that is called false positive.

False positives are a significant problem for static analysis tools. Programmers need
to spend time investigating them and consider it a waste of time when they discover
that a ``bug'' is a not actually a bug.   Future runs of the tool still report
the potential defect, so programmers need to suppress or ignore the defect.
More problematically, programmers cannot distinguish easily between false positives
and true positives, so they  ignore or suppress genuine bugs \cite{Bessey2010}.
Tools themselves may suppress error messages for true positives to avoid
too many false positives.

Most static analysis tools make unsound assumptions about C programs.
For example, they may inspect only a limited number of paths through
a function \cite{Bush2000}.  They may also assume that signed integer arithmetic
expressions in C do not overflow.  This is because it would be very difficult to
prove in general that C arithmetic stays within bounds.

There are a few sound static analysis tools that aim to detect all
possible out-of-bounds memory accesses in C programs with few false positives.
Astr\'{e}e \cite{Astree2016,Blanchet2003,Delmas2007}
and Polyspace Code Prover \cite{Polyspace2016} use 
abstract interpretation and other static analyses to diagnose possible integer
overflows, division-by-zero, and out-of-bounds
poointers in embedded C and C++ software.  They can be used to show that a program is 
free from runtime errors. They each produce output that classifies code as free of
runtime errors, definitely faulty, definitely unreachable, or possibly containing a runtime error.  They are ``black boxes'': a programmer must trust the output of the
tool.  In the case of Astr\'{e}e,  extensive detail about its analyses are available,
but one must still trust its implementation.  In the case of Polyspace, details
about the analyses are not available.

To summarize, static analysis tools may miss errors and report errors that do not exist
or errors that are ignored by programmers.  They also function as ``black
boxes'' not subject to independent verification.

Checked C occupies a different design point than static analysis tools.
First, it does not miss bounds errors for checked pointer types. 
It does static checking of bounds information in a sound fashion for sequential
C programs.  It inserts runtime checks at all uses of checked pointer unless
checks are proven to be unecessary.  Second, it avoids problems with false positives
by deferring bounds checks to runtime.  It minimizes the information that
must be checked statically by limiting it to bounds information.
Third, for cases where bounds checking failures are unacceptable, it allows
programmers to opt-in to completely static checking.  Finally, the checking is
done using rules that are part of the language definition.

\section{Program verification}

Static analysis tools take programs ``as is'' and try to prove
facts about them.  Sound static analysis tools that produce no or few false
positives act as program verifiers, of course.  Program verification 
adds a specification of program behavior and programmer involvement
in constructing a proof of that the program satisfies the specification.
Program verification efforts range in practice from simple efforts to
show that a program is free from runtime faults to costly effort 
efforts to show full functional correctness.

Checked C aims at a level that does not even exist for most higher-level
languages, where bounds checking is a built-in part of the language
implementation.  The goal is to show that information for checking runtime
faults (that is, bounds information) is correct. Checked C also aims to
support showing parts of programs are free of bounds failures or null
check failures (for performance or correctness reasons).

Extended static checking \cite{Flanagan2002} aims to show that programs are 
free of runtime errors such as null check failures and array bounds errors
and free of concurrency errors.  ESC/Java \cite{Flanagan2002}
uses a modular checking approach
where parts of the program (methods) are checked individually.  This enables 
the checking to scale to large code bases or separately compiled code.
In ESC/Java, a programmer add annotations in first-order logic to  Java programs to specify invariants 
that assist in showing the correctness of programs.  ESC/Java generates verification
conditions for functions that imply that statements with runtime checks in the
functions will not fail. It then invokes a theorem prover to attempt to 
show that the verification condition is true.  The theorem prover may construct
a proof, construct a counterexample, or not terminate within a desired time bound.
ESC/Java is unsound; it ignores integer overflow, for example.  ESC/Java uses 
a limited language of annotations and is not suitable for showing full correctness.

Checked C, like ESC/Java, uses a modular checking approach.  It checks 
individual functions.  Checked C is intended to be sound for single-threaded 
programs.  Checked C does not
provide quantifiers in the language of assertions and uses a propositional
logic, instead of a first-order logic. Given that variables may only have 
finite-sized values, it is decidable whether program invariants for Checked C 
functions are true.  However, it is an NP-complete problem and  Checked C uses
heuristics to check program invariants.

\section{Programming languages}

The Cyclone, CCured, and Deputy projects have proposed new type-safe dialects 
of C.   Cyclone \cite{Jim2002} is a dialect of C that restricts C and also extends it.  The goal is to create a type-safe system programming
language.  Cyclone has language changes that break existing C programs and
cause them to no longer to compile.  For example, Cyclone does not allow pointer
arithmetic on unchecked C types.   In contrast, Checked C supersets C. This
makes it possible to use Checked C extensions incrementally, while Cyclone
requires that a program to be converted in it entirely.  Cyclone changes the representation of pointer types that allow arithmetic.  It introduces a new 
pointer type that carries bounds information with it, similar to the \spanptr\
in Checked C. The causes compatibility problems when interoperating with existing 
code.   Checked C allows programmers to declare bounds information separately from pointers for \arrayptr\ types.
Cyclone also changes memory management in C.  It extends C with regions
to allow arena-based memory management.  Checked C is addressing the
safety problems in C one at a time, starting with bounds checking. 

CCured \cite{Necula2005} uses whole-program static analysis to identify 
different uses of pointers in C programs. It identifies pointers that are used to
read or write values only (safe pointers), pointers that are used in pointer
arithmetic also (sequence pointers), and pointers that are involved in
possibly non-type safe casts (wild pointers). It uses a multi-machine
word representation for sequence pointers and wild pointers. It also
changes the representation of data pointed to by wild pointers. This
changes data layouts and causes interoperation problems.

Deputy \cite{Condit2007,Feng2006} extends C with dependent types to avoid runtime 
layout changes for pointers involved pointer arithmetic.  Checked C is directly
inspired by Deputy, although Checked C uses program invariants instead of
dependent types to track bounds information.   The dependent types in Checked C
allow the bounds of
the pointers to be specified as part of the types of the pointers and
the bounds to depend on runtime values. Deputy requires programmers to
annotate function parameters, data structures, and global parameters
with dependent types. It then infers dependent type annotations for
local variables and adds runtime checks to make the dependently-typed
program type check. The runtime checks enforce that pointer values stay
in bounds. The checks apply to pointer arithmetic and pointer
dereferencing.

A dependent type is a type that may depend on a value at runtime.
Dependent types are built using type constructors that are applied to
types and values. The type constructors capture specific properties of
runtime values. For example, Deputy introduces a type constructor
\verb|Array| that can be applied to an integer value (the length of the
array) and an element type. \verb|Array 5 int| describes the type of
integer arrays with 5 elements. The \verb|Array| type constructor can be
applied to a program variable or an expression, so the type can depend
on a runtime value. For example, \verb|Array n int| describes the type
of integer arrays with n elements.

There are several problems with using dependent types in C.
First, dependent types are a big change to the
C type system and C type checking. Dependent types are an abstract
concept that may be hard for many programmers to understand. 
Second, even if
programmers can understand dependent types, type checking now becomes a
complicated exercise: to type check a dependently-typed statement, the
type checker must prove that certain runtime invariants are true before
the statement. To illustrate this, consider the type checking rule for
variable assignment from \cite{Condit2007}. This
rule requires that the type checker prove that certain invariants must
be true at runtime before the statement. Type checking becomes entangled
in general reasoning about program invariants.
Finally, using dependent types makes programs verbose: explicit
checks to enforce bounds safety have to be inserted through out the code. There
are no widely-used languages with array-bounds checking
that requires that level of verbosity. In Java and C\#, the checks are
done implicitly. This is the case in older languages such as FORTRAN
and Pascal, as well.  For these reasons, Checked C uses program invariants
instead of dependent types.

Havoc \cite{Condit2009} goes beyond Deputy and allows types to be combined with program
verification. It allows a programmer to specify program invariants that
imply type safety and can verify these invariants statically. It can
handle unsafe code such as using a pointer to a field to access a prior
field in a data structure.    Havoc shows that it can be very difficult
to show the type safety statically of  low-level systems code. 
The code may be type-safe at runtime and a programmer may intuitively know that it is type-safe.  However, writing down the invariants may be hard and may require deep
knowledge of program verification techniques. For this reason,
Checked C adopts an opt-in model for bounds checking, where code
can be incrementally modified to use bounds checking.

Yarra \cite{Schlesinger2011} proposes an extension to C to prevent data
modification and data disclosure attacks involving important data. 
Programmers can declare types as ``critical'' types.  All other types
are ``non-critical'' types.  A pointer to a critical type \var{T} can 
be used to access only objects with runtime type \var{T}.   Conversely, pointers
to non-critical types cannot be used to access critical objects. 
The locations and types of objects of critical types are 
tracked at runtime.  Runtime checks protect critical objects from modification
by out-of-bounds memory writes.  If the source code for a program and the libraries
it uses are available, they can be be recompiled with runtime checks at every memory
access.  Yarra places objects of critical type in separate areas of memory from
non-critical types. For calls to unmodified libraries, Yarra uses virtual memory
protection to prevent the libraries from accessing critical data.

Failure-oblivious computing \cite{Rinard2004} proposes different approach for
handling out-of-bounds memory accesses than what is propposed for Checked C.  
With Checked C,
the program will be terminated or an error handler will be invoked.  If the
program terminates, this converts data modification and data disclosure attacks to denial-of-service attacks.  Rinard {\it et al.} suggest discarding out-of-bounds
writes and converting out-of-bounds reads to small integer values, cycling
through a sequence of small integer values.   They implement the runtime
bounds checking suggested by \cite{Ruwase2004} and show that their approach
approach improves the availablity of servers with memory 
corruption errors.

